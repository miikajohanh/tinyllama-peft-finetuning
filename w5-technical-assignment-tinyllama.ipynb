{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:55:16.676657Z","iopub.execute_input":"2025-02-18T19:55:16.676928Z","iopub.status.idle":"2025-02-18T19:55:17.650252Z","shell.execute_reply.started":"2025-02-18T19:55:16.676893Z","shell.execute_reply":"2025-02-18T19:55:17.649323Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q -U bitsandbytes transformers peft accelerate datasets scipy einops evaluate trl rouge_score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:55:35.311110Z","iopub.execute_input":"2025-02-18T19:55:35.311391Z","iopub.status.idle":"2025-02-18T19:55:59.652423Z","shell.execute_reply.started":"2025-02-18T19:55:35.311370Z","shell.execute_reply":"2025-02-18T19:55:59.651337Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m342.1/342.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\nydata-profiling 4.12.1 requires scipy<1.14,>=1.4.1, but you have scipy 1.15.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import bitsandbytes\nimport transformers\nimport peft\nimport accelerate\nimport datasets\nimport scipy\nimport einops\nimport evaluate\nimport trl\nimport rouge_score\n\nprint(\"BitsAndBytes version:\", bitsandbytes.__version__)\nprint(\"Transformers version:\", transformers.__version__)\nprint(\"PEFT version:\", peft.__version__)\nprint(\"Accelerate version:\", accelerate.__version__)\nprint(\"Datasets version:\", datasets.__version__)\nprint(\"Scipy version:\", scipy.__version__)\nprint(\"Einops version:\", einops.__version__)\nprint(\"Evaluate version:\", evaluate.__version__)\nprint(\"TRL version:\", trl.__version__)\n# print(\"Rouge Score version:\", rouge_score.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:56:04.664029Z","iopub.execute_input":"2025-02-18T19:56:04.664476Z","iopub.status.idle":"2025-02-18T19:56:25.283187Z","shell.execute_reply.started":"2025-02-18T19:56:04.664432Z","shell.execute_reply":"2025-02-18T19:56:25.282387Z"}},"outputs":[{"name":"stdout","text":"BitsAndBytes version: 0.45.2\nTransformers version: 4.49.0\nPEFT version: 0.14.0\nAccelerate version: 1.4.0\nDatasets version: 3.3.1\nScipy version: 1.15.2\nEinops version: 0.8.1\nEvaluate version: 0.4.3\nTRL version: 0.15.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\n# disable Weights and Biases\nos.environ['WANDB_DISABLED']=\"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:56:32.999049Z","iopub.execute_input":"2025-02-18T19:56:32.999674Z","iopub.status.idle":"2025-02-18T19:56:33.003614Z","shell.execute_reply.started":"2025-02-18T19:56:32.999645Z","shell.execute_reply":"2025-02-18T19:56:33.002811Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    GenerationConfig\n)\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nimport torch\nimport time\nimport pandas as pd\nimport numpy as np\nfrom huggingface_hub import interpreter_login\n\ninterpreter_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:56:39.891127Z","iopub.execute_input":"2025-02-18T19:56:39.891444Z","iopub.status.idle":"2025-02-18T19:56:52.246360Z","shell.execute_reply.started":"2025-02-18T19:56:39.891414Z","shell.execute_reply":"2025-02-18T19:56:52.245663Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"\n    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your token (input will not be visible):  Â·Â·Â·Â·Â·Â·Â·Â·\nAdd token as git credential? (Y/n)  n\n"}],"execution_count":5},{"cell_type":"code","source":"from pynvml import *\n\ndef print_gpu_utilization():\n    nvmlInit()\n    handle = nvmlDeviceGetHandleByIndex(0)\n    info = nvmlDeviceGetMemoryInfo(handle)\n    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:57:55.648768Z","iopub.execute_input":"2025-02-18T19:57:55.649149Z","iopub.status.idle":"2025-02-18T19:57:55.653487Z","shell.execute_reply.started":"2025-02-18T19:57:55.649120Z","shell.execute_reply":"2025-02-18T19:57:55.652608Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# https://huggingface.co/datasets/neil-code/dialogsum-test\nhuggingface_dataset_name = \"neil-code/dialogsum-test\"\ndataset = load_dataset(huggingface_dataset_name)\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:58:19.489564Z","iopub.execute_input":"2025-02-18T19:58:19.489853Z","iopub.status.idle":"2025-02-18T19:58:25.448077Z","shell.execute_reply.started":"2025-02-18T19:58:19.489833Z","shell.execute_reply":"2025-02-18T19:58:25.447375Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.56k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48e538bdcb634607a672ddca58812e19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/1.81M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4558f48f174a4f4fb74b7e0d443ba69e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.csv:   0%|          | 0.00/441k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c5b6b93c8c24ef496df94582231834e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/447k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfe668cadab94eab89045c97fbf1636c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4646987b755473fbeb769b144405d65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7db83c1521484a5c9973080b51d37386"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"404a4e3043db4efb9b50613e640216ce"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 1999\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary', 'topic'],\n        num_rows: 499\n    })\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"dataset['train'][0]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:58:48.073571Z","iopub.execute_input":"2025-02-18T19:58:48.073908Z","iopub.status.idle":"2025-02-18T19:58:48.080749Z","shell.execute_reply.started":"2025-02-18T19:58:48.073859Z","shell.execute_reply":"2025-02-18T19:58:48.079932Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'id': 'train_0',\n 'dialogue': \"#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\\n#Person2#: I found it would be a good idea to get a check-up.\\n#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\\n#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\\n#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\\n#Person2#: Ok.\\n#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\\n#Person2#: Yes.\\n#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\\n#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\\n#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\\n#Person2#: Ok, thanks doctor.\",\n 'summary': \"Mr. Smith's getting a check-up, and Doctor Hawkins advises him to have one every year. Hawkins'll give some information about their classes and medications to help Mr. Smith quit smoking.\",\n 'topic': 'get a check-up'}"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"compute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type='nf4',\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=False,\n    )\ndevice_map = {\"\": 0}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T19:59:09.641917Z","iopub.execute_input":"2025-02-18T19:59:09.642210Z","iopub.status.idle":"2025-02-18T19:59:09.647901Z","shell.execute_reply.started":"2025-02-18T19:59:09.642190Z","shell.execute_reply":"2025-02-18T19:59:09.647144Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"model_name='TinyLlama/TinyLlama-1.1B-Chat-v1.0'\ndevice = torch.device(\"cuda:0\")\n\noriginal_model = AutoModelForCausalLM.from_pretrained(model_name, \n                                                      device_map=device_map,\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:01:31.992398Z","iopub.execute_input":"2025-02-18T20:01:31.992739Z","iopub.status.idle":"2025-02-18T20:02:28.705849Z","shell.execute_reply.started":"2025-02-18T20:01:31.992711Z","shell.execute_reply":"2025-02-18T20:02:28.705211Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56198d2d4beb421f8b7f404fefc8288b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52882a4f55004fe298920e32bc4407d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b196e4bd91114bdbbba4b6e92ca30216"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa\ntokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False, use_auth_token=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:05:15.440954Z","iopub.execute_input":"2025-02-18T20:05:15.441381Z","iopub.status.idle":"2025-02-18T20:05:26.520903Z","shell.execute_reply.started":"2025-02-18T20:05:15.441343Z","shell.execute_reply":"2025-02-18T20:05:26.519955Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:833: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c68df9da031b4724a6e06168b2213d42"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"print_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:05:31.892593Z","iopub.execute_input":"2025-02-18T20:05:31.893067Z","iopub.status.idle":"2025-02-18T20:05:31.898331Z","shell.execute_reply.started":"2025-02-18T20:05:31.893024Z","shell.execute_reply":"2025-02-18T20:05:31.897373Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 1224 MB.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False, use_auth_token=True)\neval_tokenizer.pad_token = eval_tokenizer.eos_token\n\ndef gen(model,p, maxlen=100, sample=True):\n    toks = eval_tokenizer(p, return_tensors=\"pt\").to(\"cuda:0\")\n    res = model.generate(**toks.to(\"cuda\"), max_new_tokens=maxlen, do_sample=sample,num_return_sequences=1,temperature=0.1,num_beams=1,top_p=0.95,).to('cpu')\n    return eval_tokenizer.batch_decode(res,skip_special_tokens=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:06:18.407066Z","iopub.execute_input":"2025-02-18T20:06:18.407357Z","iopub.status.idle":"2025-02-18T20:06:18.804172Z","shell.execute_reply.started":"2025-02-18T20:06:18.407335Z","shell.execute_reply":"2025-02-18T20:06:18.803218Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nseed = 42\nset_seed(seed)\n\nindex = 10\n\nprompt = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nformatted_prompt = f\"Instruct: Summarize the following conversation.\\n{prompt}\\nOutput:\\n\"\nres = gen(original_model,formatted_prompt,100,)\n#print(res[0])\noutput = res[0].split('Output:\\n')[1]\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{formatted_prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'MODEL GENERATION - ZERO SHOT:\\n{output}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:06:26.410350Z","iopub.execute_input":"2025-02-18T20:06:26.410773Z","iopub.status.idle":"2025-02-18T20:06:30.105653Z","shell.execute_reply.started":"2025-02-18T20:06:26.410735Z","shell.execute_reply":"2025-02-18T20:06:30.104756Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nMODEL GENERATION - ZERO SHOT:\nBrian: Happy Birthday, this is for you, Brian.\nPerson2: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\nPerson1: Brian, may I have a pleasure to have a dance with you?\nPerson2: Ok.\nPerson1: This is really wonderful party.\nPerson2: Yes, you are always popular with everyone. and you\nCPU times: user 3.21 s, sys: 43.8 ms, total: 3.26 s\nWall time: 3.69 s\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def create_prompt_formats(sample):\n    \"\"\"\n    Format various fields of the sample ('instruction','output')\n    Then concatenate them using two newline characters \n    :param sample: Sample dictionnary\n    \"\"\"\n    INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    INSTRUCTION_KEY = \"### Instruct: Summarize the below conversation.\"\n    RESPONSE_KEY = \"### Output:\"\n    END_KEY = \"### End\"\n    \n    blurb = f\"\\n{INTRO_BLURB}\"\n    instruction = f\"{INSTRUCTION_KEY}\"\n    input_context = f\"{sample['dialogue']}\" if sample[\"dialogue\"] else None\n    response = f\"{RESPONSE_KEY}\\n{sample['summary']}\"\n    end = f\"{END_KEY}\"\n    \n    parts = [part for part in [blurb, instruction, input_context, response, end] if part]\n\n    formatted_prompt = \"\\n\\n\".join(parts)\n    sample[\"text\"] = formatted_prompt\n\n    return sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:06:52.383508Z","iopub.execute_input":"2025-02-18T20:06:52.383802Z","iopub.status.idle":"2025-02-18T20:06:52.388686Z","shell.execute_reply.started":"2025-02-18T20:06:52.383779Z","shell.execute_reply":"2025-02-18T20:06:52.387866Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef get_max_length(model):\n    conf = model.config\n    max_length = None\n    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n        max_length = getattr(model.config, length_setting, None)\n        if max_length:\n            print(f\"Found max lenth: {max_length}\")\n            break\n    if not max_length:\n        max_length = 1024\n        print(f\"Using default max length: {max_length}\")\n    return max_length\n\n\ndef preprocess_batch(batch, tokenizer, max_length):\n    \"\"\"\n    Tokenizing a batch\n    \"\"\"\n    return tokenizer(\n        batch[\"text\"],\n        max_length=max_length,\n        truncation=True,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:06:57.977028Z","iopub.execute_input":"2025-02-18T20:06:57.977340Z","iopub.status.idle":"2025-02-18T20:06:57.982525Z","shell.execute_reply.started":"2025-02-18T20:06:57.977316Z","shell.execute_reply":"2025-02-18T20:06:57.981624Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from functools import partial\n\n# SOURCE https://github.com/databrickslabs/dolly/blob/master/training/trainer.py\ndef preprocess_dataset(tokenizer: AutoTokenizer, max_length: int,seed, dataset):\n    \"\"\"Format & tokenize it so it is ready for training\n    :param tokenizer (AutoTokenizer): Model Tokenizer\n    :param max_length (int): Maximum number of tokens to emit from tokenizer\n    \"\"\"\n    \n    # Add prompt to each sample\n    print(\"Preprocessing dataset...\")\n    dataset = dataset.map(create_prompt_formats)#, batched=True)\n    \n    _preprocessing_function = partial(preprocess_batch, max_length=max_length, tokenizer=tokenizer)\n    dataset = dataset.map(\n        _preprocessing_function,\n        batched=True,\n        remove_columns=['id', 'topic', 'dialogue', 'summary'],\n    )\n\n    # Filter out samples that have input_ids exceeding max_length\n    dataset = dataset.filter(lambda sample: len(sample[\"input_ids\"]) < max_length)\n    \n    # Shuffle dataset\n    dataset = dataset.shuffle(seed=seed)\n\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:07:05.807915Z","iopub.execute_input":"2025-02-18T20:07:05.808210Z","iopub.status.idle":"2025-02-18T20:07:05.813422Z","shell.execute_reply.started":"2025-02-18T20:07:05.808188Z","shell.execute_reply":"2025-02-18T20:07:05.812540Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"print_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:07:10.663221Z","iopub.execute_input":"2025-02-18T20:07:10.663540Z","iopub.status.idle":"2025-02-18T20:07:10.668484Z","shell.execute_reply.started":"2025-02-18T20:07:10.663512Z","shell.execute_reply":"2025-02-18T20:07:10.667614Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 1286 MB.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# ## Pre-process dataset\nmax_length = get_max_length(original_model)\nprint(max_length)\n\ntrain_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['train'])\neval_dataset = preprocess_dataset(tokenizer, max_length,seed, dataset['validation'])\n\nprint(f\"Shapes of the datasets:\")\nprint(f\"Training: {train_dataset.shape}\")\nprint(f\"Validation: {eval_dataset.shape}\")\nprint(train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:07:15.810044Z","iopub.execute_input":"2025-02-18T20:07:15.810348Z","iopub.status.idle":"2025-02-18T20:07:20.819472Z","shell.execute_reply.started":"2025-02-18T20:07:15.810326Z","shell.execute_reply":"2025-02-18T20:07:20.818790Z"}},"outputs":[{"name":"stdout","text":"Found max lenth: 2048\n2048\nPreprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46c34568cd87471890813b87fd2f7df1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"733f7607a8994937af45385c76510f77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1999 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21f5f38129c5403da3091e179441eb82"}},"metadata":{}},{"name":"stdout","text":"Preprocessing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e174004a0aa34d3d831afc4a83f2417a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63408c999eb44889bf28ec39c51f3c75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/499 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"848c4f923e87450090593e74108f855c"}},"metadata":{}},{"name":"stdout","text":"Shapes of the datasets:\nTraining: (1999, 3)\nValidation: (499, 3)\nDataset({\n    features: ['text', 'input_ids', 'attention_mask'],\n    num_rows: 1999\n})\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:07:27.693378Z","iopub.execute_input":"2025-02-18T20:07:27.693688Z","iopub.status.idle":"2025-02-18T20:07:27.700071Z","shell.execute_reply.started":"2025-02-18T20:07:27.693663Z","shell.execute_reply":"2025-02-18T20:07:27.699110Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 131164160\nall model parameters: 615606272\npercentage of trainable model parameters: 21.31%\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(original_model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:07:32.203381Z","iopub.execute_input":"2025-02-18T20:07:32.203683Z","iopub.status.idle":"2025-02-18T20:07:32.209327Z","shell.execute_reply.started":"2025-02-18T20:07:32.203661Z","shell.execute_reply":"2025-02-18T20:07:32.208408Z"}},"outputs":[{"name":"stdout","text":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 2048)\n    (layers): ModuleList(\n      (0-21): 22 x LlamaDecoderLayer(\n        (self_attn): LlamaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nconfig = LoraConfig(\n    r=32, #Rank\n    lora_alpha=32,\n    target_modules=[\n        'q_proj',\n        'k_proj',\n        'v_proj',\n        'dense'\n    ],\n    bias=\"none\",\n    lora_dropout=0.05,  # Conventional\n    task_type=\"CAUSAL_LM\",\n)\n\n# 1 - Enabling gradient checkpointing to reduce memory usage during fine-tuning\noriginal_model.gradient_checkpointing_enable()\n\n# 2 - Using the prepare_model_for_kbit_training method from PEFT\noriginal_model = prepare_model_for_kbit_training(original_model)\n\npeft_model = get_peft_model(original_model, config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:07:47.122252Z","iopub.execute_input":"2025-02-18T20:07:47.122545Z","iopub.status.idle":"2025-02-18T20:07:47.284898Z","shell.execute_reply.started":"2025-02-18T20:07:47.122523Z","shell.execute_reply":"2025-02-18T20:07:47.283932Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:07:54.804224Z","iopub.execute_input":"2025-02-18T20:07:54.804524Z","iopub.status.idle":"2025-02-18T20:07:54.811307Z","shell.execute_reply.started":"2025-02-18T20:07:54.804501Z","shell.execute_reply":"2025-02-18T20:07:54.810382Z"}},"outputs":[{"name":"stdout","text":"trainable model parameters: 6127616\nall model parameters: 621733888\npercentage of trainable model parameters: 0.99%\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"output_dir = './peft-dialogue-summary-training/final-checkpoint'\nimport transformers\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    warmup_steps=1,\n    per_device_train_batch_size=4,  # âœ… Increased for better GPU usage\n    gradient_accumulation_steps=2,  # âœ… Reduced for speed\n    max_steps=500,\n    learning_rate=2e-4,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=50,  # âœ… Logs less often\n    save_strategy=\"steps\",\n    save_steps=50,  # âœ… Saves less often\n    evaluation_strategy=\"steps\",\n    eval_steps=50,  # âœ… Evaluates less often\n    do_eval=True,\n    gradient_checkpointing=False,  # âœ… Disabled for speed\n    fp16=True,  # âœ… Enables mixed precision\n    report_to=\"none\",\n    overwrite_output_dir=True,\n    group_by_length=True,\n)\n\n\npeft_model.config.use_cache = False\n\npeft_trainer = transformers.Trainer(\n    model=peft_model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=peft_training_args,\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:08:49.395868Z","iopub.execute_input":"2025-02-18T20:08:49.396189Z","iopub.status.idle":"2025-02-18T20:08:49.436638Z","shell.execute_reply.started":"2025-02-18T20:08:49.396165Z","shell.execute_reply":"2025-02-18T20:08:49.435846Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\nNo label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"peft_training_args.device\npeft_trainer.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:09:05.426978Z","iopub.execute_input":"2025-02-18T20:09:05.427315Z","iopub.status.idle":"2025-02-18T20:43:09.821018Z","shell.execute_reply.started":"2025-02-18T20:09:05.427291Z","shell.execute_reply":"2025-02-18T20:43:09.820132Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 33:52, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.433900</td>\n      <td>1.332374</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.256900</td>\n      <td>1.312465</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.288000</td>\n      <td>1.283109</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.267900</td>\n      <td>1.273435</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.275600</td>\n      <td>1.272369</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.237800</td>\n      <td>1.266189</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.250600</td>\n      <td>1.262266</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.233700</td>\n      <td>1.260827</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.224300</td>\n      <td>1.260158</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.232500</td>\n      <td>1.259174</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=1.2700997085571288, metrics={'train_runtime': 2043.2397, 'train_samples_per_second': 1.958, 'train_steps_per_second': 0.245, 'total_flos': 7998465185280000.0, 'train_loss': 1.2700997085571288, 'epoch': 2.0})"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()  # Enter your HF token\n\n# Upload Model & Tokenizer\npeft_model.push_to_hub(\"miikajohanh/tinyllama-dialogsum-finetuned\", private=True)\ntokenizer.push_to_hub(\"miikajohanh/tinyllama-dialogsum-finetuned\", private=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:43:26.462599Z","iopub.execute_input":"2025-02-18T20:43:26.463057Z","iopub.status.idle":"2025-02-18T20:43:36.832932Z","shell.execute_reply.started":"2025-02-18T20:43:26.463021Z","shell.execute_reply":"2025-02-18T20:43:36.832147Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb93eeaedd5f4726ba977e07560f780f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/24.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e84501e1a2c94d689f4a7c5245994f04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5b6439367f240f2b3c805cbf99d1b81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a501c7894d93430daffaf258d8df62c4"}},"metadata":{}},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/miikajohanh/tinyllama-dialogsum-finetuned/commit/bfa6f9985b0c62954af9d6ce22b4bafea70f3e9a', commit_message='Upload tokenizer', commit_description='', oid='bfa6f9985b0c62954af9d6ce22b4bafea70f3e9a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/miikajohanh/tinyllama-dialogsum-finetuned', endpoint='https://huggingface.co', repo_type='model', repo_id='miikajohanh/tinyllama-dialogsum-finetuned'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"print_gpu_utilization()\ndel original_model\ndel peft_trainer\ntorch.cuda.empty_cache()\nprint_gpu_utilization()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:44:00.582341Z","iopub.execute_input":"2025-02-18T20:44:00.582677Z","iopub.status.idle":"2025-02-18T20:44:00.705189Z","shell.execute_reply.started":"2025-02-18T20:44:00.582645Z","shell.execute_reply":"2025-02-18T20:44:00.704262Z"}},"outputs":[{"name":"stdout","text":"GPU memory occupied: 7786 MB.\nGPU memory occupied: 1610 MB.\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nbase_model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nbase_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:44:37.551256Z","iopub.execute_input":"2025-02-18T20:44:37.551556Z","iopub.status.idle":"2025-02-18T20:44:40.428883Z","shell.execute_reply.started":"2025-02-18T20:44:37.551530Z","shell.execute_reply":"2025-02-18T20:44:40.428232Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True, use_fast=False, token=True)\neval_tokenizer.pad_token = eval_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:44:43.242014Z","iopub.execute_input":"2025-02-18T20:44:43.242316Z","iopub.status.idle":"2025-02-18T20:44:43.631218Z","shell.execute_reply.started":"2025-02-18T20:44:43.242294Z","shell.execute_reply":"2025-02-18T20:44:43.630244Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"from peft import PeftModel\n\nft_model = PeftModel.from_pretrained(base_model, \"/kaggle/working/peft-dialogue-summary-training/final-checkpoint/checkpoint-500\",torch_dtype=torch.float16,is_trainable=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:46:07.591195Z","iopub.execute_input":"2025-02-18T20:46:07.591488Z","iopub.status.idle":"2025-02-18T20:46:07.781634Z","shell.execute_reply.started":"2025-02-18T20:46:07.591466Z","shell.execute_reply":"2025-02-18T20:46:07.780955Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"%%time\nfrom transformers import set_seed\nset_seed(seed)\n\nindex = 10\ndialogue = dataset['test'][index]['dialogue']\nsummary = dataset['test'][index]['summary']\n\nprompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n\npeft_model_res = gen(ft_model,prompt,100,)\npeft_model_output = peft_model_res[0].split('Output:\\n')[1]\n#print(peft_model_output)\nprefix, success, result = peft_model_output.partition('#End')\n\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\nprint(dash_line)\nprint(f'PEFT MODEL:\\n{prefix}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:46:18.933309Z","iopub.execute_input":"2025-02-18T20:46:18.933627Z","iopub.status.idle":"2025-02-18T20:46:23.277388Z","shell.execute_reply.started":"2025-02-18T20:46:18.933600Z","shell.execute_reply":"2025-02-18T20:46:23.276701Z"}},"outputs":[{"name":"stdout","text":"---------------------------------------------------------------------------------------------------\nINPUT PROMPT:\nInstruct: Summarize the following conversation.\n#Person1#: Happy Birthday, this is for you, Brian.\n#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n#Person1#: Brian, may I have a pleasure to have a dance with you?\n#Person2#: Ok.\n#Person1#: This is really wonderful party.\n#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n#Person2#: You look great, you are absolutely glowing.\n#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\nOutput:\n\n---------------------------------------------------------------------------------------------------\nBASELINE HUMAN SUMMARY:\n#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n\n---------------------------------------------------------------------------------------------------\nPEFT MODEL:\n#Person1# invites Brian to the party and thanks him for the birthday party.\n\n###### Step 4: End the conversation\n\n#Person1#: Thanks for the party, Brian. I hope you have a good time.\n#Person2#: Thanks, I'm glad you enjoyed it.\n#Person1#: Bye.\n#Person2#: Bye.\n\n###### End the conversation.\n\n\nCPU times: user 4.32 s, sys: 1.47 ms, total: 4.33 s\nWall time: 4.34 s\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"original_model = AutoModelForCausalLM.from_pretrained(base_model_id, \n                                                      device_map='auto',\n                                                      quantization_config=bnb_config,\n                                                      trust_remote_code=True,\n                                                      use_auth_token=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:46:37.885331Z","iopub.execute_input":"2025-02-18T20:46:37.885637Z","iopub.status.idle":"2025-02-18T20:46:40.740056Z","shell.execute_reply.started":"2025-02-18T20:46:37.885611Z","shell.execute_reply":"2025-02-18T20:46:40.739401Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"import pandas as pd\n\ndialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    human_baseline_text_output = human_baseline_summaries[idx]\n    prompt = f\"Instruct: Summarize the following conversation.\\n{dialogue}\\nOutput:\\n\"\n    \n    original_model_res = gen(original_model,prompt,100,)\n    original_model_text_output = original_model_res[0].split('Output:\\n')[1]\n    \n    peft_model_res = gen(ft_model,prompt,100,)\n    peft_model_output = peft_model_res[0].split('Output:\\n')[1]\n    #print(peft_model_output)\n    peft_model_text_output, success, result = peft_model_output.partition('#End')\n    \n\n    original_model_summaries.append(original_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'peft_model_summaries'])\n\nfrom IPython.display import display\n\n#pd.set_option('display.max_columns', None)  # Show all columns\n#pd.set_option('display.max_rows', None)     # Show all rows\npd.set_option('display.max_colwidth', None) # Show full content of each column\n\ndisplay(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:53:26.454426Z","iopub.execute_input":"2025-02-18T20:53:26.454762Z","iopub.status.idle":"2025-02-18T20:54:28.120472Z","shell.execute_reply.started":"2025-02-18T20:53:26.454732Z","shell.execute_reply":"2025-02-18T20:54:28.119659Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                                                                                                                                                                          human_baseline_summaries  \\\n0                                              Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.   \n1  In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.   \n2                                  Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.   \n3                                                       #Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.   \n4                                                                                      #Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.   \n5                                                                            #Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.   \n6                                                                                            #Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.   \n7                                                                                         #Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.   \n8                                                                                 #Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched   \n9                                                                                                       #Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                        original_model_summaries  \\\n0                                                                                                                   #Person1#: Ms. Dawson, I need you to take a dictation for me. #Person2#: Yes, sir... #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. #Person2#: Yes, sir. Go ahead. #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of   \n1  #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications? #Person1#: It should apply to all communications, not only in this office between employees, but also any outside communic   \n2  #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications? #Person1#: It should apply to all communications, not only in this office between employees, but also any outside communic   \n3                                                                                                                                                                                                                                                                                                                                                                 - The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.   \n4                                                                                                                                                                                                                                                                                                                                                                 - The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.   \n5                                                                                                                                                                                                                                                                                                                                                                 - The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.   \n6                                                                                              - Summarize the conversation between two people.\\n- The conversation mentions the divorce of Masha and Hero, and the possibility of a separation for 2 months, and the possibility of the kids getting custody. The conversation also mentions that the divorce is not a surprise, and that the kids will get custody. The conversation also mentions that the divorce is final in early January.   \n7                                                                                                                                                                                                                                                                                                                                                                                                                                         - The conversation between two people about a divorce.   \n8                                                                                                                                                                                                                                                                     - The conversation between two friends, one of them is a divorce lawyer, and the other is a divorced person.\\n- The conversation between two friends, one of them is a divorce lawyer, and the other is a divorced person.   \n9                                                                                                                                  Happy Birthday, Brian.\\nI'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\\nBrian, may I have a pleasure to have a dance with you?\\nOk.\\nThis is really wonderful party.\\nYes, you are always popular with everyone.\\nand you look very pretty today.\\nThanks, that's very kind of you to say.\\nI   \n\n                                                                                                                                                                                                                                                                                                                                                                                                        peft_model_summaries  \n0         #Person1# dictates an intra-office memorandum to all employees.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the new policy on internal and external communications.\\n#Person1#: This applies to internal and external communications.\\n#Person2#: Yes, sir. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation  \n1  #Person1# dictates an intra-office memorandum to all employees.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the new policy regarding internal and external communications.\\n#Person1#: This applies to internal and external communications.\\n#Person2#: Yes, sir. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation  \n2                                                                                       #Person1# dictates an intra-office memorandum to all employees.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1#: Ms. Dawson, I need you to take a dictation for me.\\n#Person2#: Yes, sir...\\n#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\\n#Person2#: Yes  \n3                                                                                                                                                                                #Person1# and #Person2# have a conversation about #Person2#'s decision to quit driving to work. #Person2# thinks it's not good for the environment and #Person1# thinks it's not good for #Person2#'s health.\\n\\n####### End of the story\\n  \n4                                                                                                                                                                                #Person1# and #Person2# have a conversation about #Person2#'s decision to quit driving to work. #Person2# thinks it's not good for the environment and #Person1# thinks it's not good for #Person2#'s health.\\n\\n####### End of the story\\n  \n5                                                                                                                                                                    #Person1# and #Person2# have a conversation about #Person2#'s decision to quit driving to work.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# have a conversation about #Person2#'s decision to quit driving to work.\\n\\n####   \n6                                                                                                                                                                                                                                #Person1# and #Person2# talk about the divorce of Masha and Hero.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# talk about the divorce of Masha and Hero.\\n\\n####   \n7                                                                                                                                                                                                                        #Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n####   \n8                                                                                                                                                                                                                        #Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n####   \n9                                                                                                                                                                                                                                                                                                                             #Person1# invites Brian to the party and thanks him for the birthday party.\\n\\n###### Step 4:   ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>human_baseline_summaries</th>\n      <th>original_model_summaries</th>\n      <th>peft_model_summaries</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Ms. Dawson helps #Person1# to write a memo to inform every employee that they have to change the communication method and should not use Instant Messaging anymore.</td>\n      <td>#Person1#: Ms. Dawson, I need you to take a dictation for me. #Person2#: Yes, sir... #Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. #Person2#: Yes, sir. Go ahead. #Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of</td>\n      <td>#Person1# dictates an intra-office memorandum to all employees.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the new policy on internal and external communications.\\n#Person1#: This applies to internal and external communications.\\n#Person2#: Yes, sir. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In order to prevent employees from wasting time on Instant Message programs, #Person1# decides to terminate the use of those programs and asks Ms. Dawson to send out a memo to all employees by the afternoon.</td>\n      <td>#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications? #Person1#: It should apply to all communications, not only in this office between employees, but also any outside communic</td>\n      <td>#Person1# dictates an intra-office memorandum to all employees.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the new policy regarding internal and external communications.\\n#Person1#: This applies to internal and external communications.\\n#Person2#: Yes, sir. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.</td>\n      <td>#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited. #Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications? #Person1#: It should apply to all communications, not only in this office between employees, but also any outside communic</td>\n      <td>#Person1# dictates an intra-office memorandum to all employees.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1#: Ms. Dawson, I need you to take a dictation for me.\\n#Person2#: Yes, sir...\\n#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\\n#Person2#: Yes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Person2# arrives late because of traffic jam. #Person1# persuades #Person2# to use public transportations to keep healthy and to protect the environment.</td>\n      <td>- The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.</td>\n      <td>#Person1# and #Person2# have a conversation about #Person2#'s decision to quit driving to work. #Person2# thinks it's not good for the environment and #Person1# thinks it's not good for #Person2#'s health.\\n\\n####### End of the story\\n</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>#Person2# decides to follow #Person1#'s suggestions on quitting driving to work and will try to use public transportations.</td>\n      <td>- The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.</td>\n      <td>#Person1# and #Person2# have a conversation about #Person2#'s decision to quit driving to work. #Person2# thinks it's not good for the environment and #Person1# thinks it's not good for #Person2#'s health.\\n\\n####### End of the story\\n</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.</td>\n      <td>- The conversation between two people discussing the pros and cons of taking public transportation instead of driving to work.</td>\n      <td>#Person1# and #Person2# have a conversation about #Person2#'s decision to quit driving to work.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# have a conversation about #Person2#'s decision to quit driving to work.\\n\\n####</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>#Person1# tells Kate that Masha and Hero get divorced. Kate is surprised because she thought they are perfect couple.</td>\n      <td>- Summarize the conversation between two people.\\n- The conversation mentions the divorce of Masha and Hero, and the possibility of a separation for 2 months, and the possibility of the kids getting custody. The conversation also mentions that the divorce is not a surprise, and that the kids will get custody. The conversation also mentions that the divorce is final in early January.</td>\n      <td>#Person1# and #Person2# talk about the divorce of Masha and Hero.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# talk about the divorce of Masha and Hero.\\n\\n####</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>#Person1# tells Kate that Masha and Hero are getting a peaceful divorce. Kate feels surprised and asks about their kids.</td>\n      <td>- The conversation between two people about a divorce.</td>\n      <td>#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n####</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>#Person1# and Kate talk about the divorce between Masha and Hero. Kate feels surprised because she thought they are well matched</td>\n      <td>- The conversation between two friends, one of them is a divorce lawyer, and the other is a divorced person.\\n- The conversation between two friends, one of them is a divorce lawyer, and the other is a divorced person.</td>\n      <td>#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n#### Instruct: Summarize the conversation.\\n#Person1# and #Person2# are discussing the divorce of Masha and Hero.\\n\\n####</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>#Person1# and Brian are at the birthday party of Brian. Brian thinks #Person1# looks great and is popular.</td>\n      <td>Happy Birthday, Brian.\\nI'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\\nBrian, may I have a pleasure to have a dance with you?\\nOk.\\nThis is really wonderful party.\\nYes, you are always popular with everyone.\\nand you look very pretty today.\\nThanks, that's very kind of you to say.\\nI</td>\n      <td>#Person1# invites Brian to the party and thanks him for the birthday party.\\n\\n###### Step 4:</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"!pip install rouge_score evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:54:50.693386Z","iopub.execute_input":"2025-02-18T20:54:50.693697Z","iopub.status.idle":"2025-02-18T20:54:54.277219Z","shell.execute_reply.started":"2025-02-18T20:54:50.693672Z","shell.execute_reply":"2025-02-18T20:54:54.276093Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.3.1)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.27.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.11.10)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.12.14)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"import evaluate\n\nrouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:54:58.746521Z","iopub.execute_input":"2025-02-18T20:54:58.746944Z","iopub.status.idle":"2025-02-18T20:55:00.930034Z","shell.execute_reply.started":"2025-02-18T20:54:58.746913Z","shell.execute_reply":"2025-02-18T20:55:00.929151Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"045cdecd3b734a35afe04f3a4b5595f1"}},"metadata":{}},{"name":"stdout","text":"ORIGINAL MODEL:\n{'rouge1': 0.2838255882138292, 'rouge2': 0.06599360042160449, 'rougeL': 0.19241705906966233, 'rougeLsum': 0.19561470972966516}\nPEFT MODEL:\n{'rouge1': 0.3053906974912508, 'rouge2': 0.0889746690786785, 'rougeL': 0.2419153228407899, 'rougeLsum': 0.2311227689039844}\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T20:55:06.012651Z","iopub.execute_input":"2025-02-18T20:55:06.012988Z","iopub.status.idle":"2025-02-18T20:55:06.018788Z","shell.execute_reply.started":"2025-02-18T20:55:06.012962Z","shell.execute_reply":"2025-02-18T20:55:06.017967Z"}},"outputs":[{"name":"stdout","text":"Absolute percentage improvement of PEFT MODEL over ORIGINAL MODEL\nrouge1: 2.16%\nrouge2: 2.30%\nrougeL: 4.95%\nrougeLsum: 3.55%\n","output_type":"stream"}],"execution_count":48}]}